\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@rmstyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand\BIBentrySTDinterwordspacing{\spaceskip=0pt\relax}
\providecommand\BIBentryALTinterwordstretchfactor{4}
\providecommand\BIBentryALTinterwordspacing{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand\BIBforeignlanguage[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}

\bibitem{Dao2020AdaptiveRL}
P.~Dao and Y.-C. Liu, ``Adaptive reinforcement learning strategy with sliding
  mode control for unknown and disturbed wheeled inverted pendulum,''
  \emph{International Journal of Control Automation and Systems}, 2020.

\bibitem{Zheng2020BalanceCF}
Y.~Zheng, X.~Li, and L.~Xu, ``Balance control for the first-order inverted
  pendulum based on the advantage actor-critic algorithm,'' \emph{International
  Journal of Control Automation and Systems}, vol.~18, pp. 1--8, 2020.

\bibitem{Xin2020RobustES}
Y.~Xin, Z.-C. Qin, and J.-Q. Sun, ``Robust experimental study of data-driven
  optimal control for an underactuated rotary flexible joint,''
  \emph{International Journal of Control, Automation and Systems}, vol.~18, pp.
  1202--1214, 2020.

\bibitem{Dornheim2018ModelfreeAO}
J.~Dornheim, N.~Link, and P.~Gumbsch, ``Model-free adaptive optimal control of
  episodic fixed-horizon manufacturing processes using reinforcement
  learning,'' \emph{International Journal of Control, Automation and Systems},
  vol.~18, pp. 1593--1604, 2018.

\bibitem{Bertsekas1996NeuroDynamicP}
D.~Bertsekas and J.~Tsitsiklis, ``Neuro-dynamic programming,'' in
  \emph{Encyclopedia of Machine Learning}, 1996.

\bibitem{Han2020ActorCriticRL}
M.~Han, L.~Zhang, J.~Wang, and W.~Pan, ``Actor-critic reinforcement learning
  for control with stability guarantee,'' \emph{IEEE Robotics and Automation
  Letters}, vol.~5, pp. 6217--6224, 2020.

\bibitem{Weinan2017APO}
E.~Weinan, ``A proposal on machine learning via dynamical systems,'' 2017.

\bibitem{Dupont2019AugmentedNO}
E.~Dupont, A.~Doucet, and Y.~Teh, ``Augmented neural odes,'' in \emph{NeurIPS},
  2019.

\bibitem{Betancourt2018OnSO}
M.~Betancourt, M.~I. Jordan, and A.~Wilson, ``On symplectic optimization,''
  \emph{arXiv: Computation}, 2018.

\bibitem{Nachum2020ReinforcementLV}
O.~Nachum and B.~Dai, ``Reinforcement learning via fenchel-rockafellar
  duality,'' \emph{ArXiv}, vol. abs/2001.01866, 2020.

\bibitem{Lv2019ApproximateOS}
Y.~Lv, X.~Ren, S.~Hu, and H.~Xu, ``Approximate optimal stabilization control of
  servo mechanisms based on reinforcement learning scheme,''
  \emph{International Journal of Control Automation and Systems}, vol.~17, pp.
  2655--2665, 2019.

\bibitem{Hewing2020LearningBasedMP}
L.~Hewing, K.~P. Wabersich, M.~Menner, and M.~N. Zeilinger, ``Learning-based
  model predictive control: Toward safe learning in control,'' 2020.

\bibitem{Mohan2020EmbeddingHP}
A.~Mohan, N.~Lubbers, D.~Livescu, and M.~Chertkov, ``Embedding hard physical
  constraints in neural network coarse-graining of 3d turbulence.''
  \emph{arXiv: Computational Physics}, 2020.

\bibitem{Lusch2018DeepLF}
B.~Lusch, J.~N. Kutz, and S.~Brunton, ``Deep learning for universal linear
  embeddings of nonlinear dynamics,'' \emph{Nature Communications}, vol.~9,
  2018.

\bibitem{Bai2019DeepEM}
S.~Bai, J.~Z. Kolter, and V.~Koltun, ``Deep equilibrium models,'' \emph{ArXiv},
  vol. abs/1909.01377, 2019.

\bibitem{BelbutePeres2020CombiningDP}
F.~de~Avila Belbute-Peres, T.~D. Economon, and J.~Z. Kolter, ``Combining
  differentiable pde solvers and graph neural networks for fluid flow
  prediction,'' \emph{ArXiv}, vol. abs/2007.04439, 2020.

\bibitem{Oh2020DeepRB}
T.-H. Oh, J.-S. Han, Y.~Kim, D.-Y. Yang, S.~Lee, and D.~D. Cho, ``Deep rl based
  notch filter design method for complex industrial servo systems,''
  \emph{International Journal of Control Automation and Systems}, vol.~18, pp.
  1--10, 2020.

\bibitem{Cheng2019OnorbitRU}
Y.~Cheng, B.~Jiang, H.~Li, and X.-D. Han, ``On-orbit reconfiguration using
  adaptive dynamic programming for multi-mission-constrained spacecraft
  attitude control system,'' \emph{International Journal of Control, Automation
  and Systems}, vol.~17, pp. 822--835, 2019.

\bibitem{Knox2009InteractivelySA}
\BIBentryALTinterwordspacing
W.~B. Knox and P.~Stone, ``Interactively shaping agents via human
  reinforcement: The tamer framework,'' in \emph{Proceedings of the Fifth
  International Conference on Knowledge Capture}, ser. K-CAP '09.\hskip 1em
  plus 0.5em minus 0.4em\relax New York, NY, USA: Association for Computing
  Machinery, 2009, p. 9–16. [Online]. Available:
  \url{https://doi.org/10.1145/1597735.1597738}
\BIBentrySTDinterwordspacing

\bibitem{Knox2010CombiningMF}
------, ``Combining manual feedback with subsequent mdp reward signals for
  reinforcement learning,'' in \emph{Proceedings of the 9th International
  Conference on Autonomous Agents and Multiagent Systems: Volume 1 - Volume 1},
  ser. AAMAS '10.\hskip 1em plus 0.5em minus 0.4em\relax Richland, SC:
  International Foundation for Autonomous Agents and Multiagent Systems, 2010,
  p. 5–12.

\bibitem{Peng2018DeepMimicED}
X.~Peng, P.~Abbeel, S.~Levine, and M.~V.~D. Panne, ``Deepmimic: Example-guided
  deep reinforcement learning of physics-based character skills,'' \emph{ACM
  Trans. Graph.}, vol.~37, pp. 143:1--143:14, 2018.

\bibitem{Peng2020LearningAR}
X.~Peng, E.~Coumans, T.~Zhang, T.~Lee, J.~Tan, and S.~Levine, ``Learning agile
  robotic locomotion skills by imitating animals,'' \emph{ArXiv}, vol.
  abs/2004.00784, 2020.

\bibitem{Paine2018OneShotHI}
T.~Paine, S.~G. Colmenarejo, Z.~Wang, S.~Reed, Y.~Aytar, T.~Pfaff, M.~W.
  Hoffman, G.~Barth-Maron, S.~Cabi, D.~Budden, and N.~D. Freitas, ``One-shot
  high-fidelity imitation: Training large-scale deep nets with rl,''
  \emph{ArXiv}, vol. abs/1810.05017, 2018.

\bibitem{Choi2017InverseRL}
S.~Choi, S.~Kim, and H.~J. Kim, ``Inverse reinforcement learning control for
  trajectory tracking of a multirotor uav,'' \emph{International Journal of
  Control, Automation and Systems}, vol.~15, pp. 1826--1834, 2017.

\bibitem{Xie2018LearningWT}
L.~Xie, S.~Wang, S.~Rosa, A.~Markham, and A.~Trigoni, ``Learning with training
  wheels: Speeding up training with a simple controller for deep reinforcement
  learning,'' \emph{2018 IEEE International Conference on Robotics and
  Automation (ICRA)}, pp. 6276--6283, 2018.

\bibitem{Carlucho2017IncrementalQS}
I.~Carlucho, M.~D. Paula, S.~A. Villar, and G.~G. Acosta, ``Incremental
  q-learning strategy for adaptive pid control of mobile robots,'' \emph{Expert
  Syst. Appl.}, vol.~80, pp. 183--199, 2017.

\bibitem{Pavse2020RIDMRI}
B.~S. Pavse, F.~Torabi, J.~P. Hanna, G.~Warnell, and P.~Stone, ``Ridm:
  Reinforced inverse dynamics modeling for learning from a single observed
  demonstration,'' \emph{IEEE Robotics and Automation Letters}, vol.~5, pp.
  6262--6269, 2020.

\bibitem{Sutton1998IntroductionTR}
R.~Sutton and A.~Barto, ``Introduction to reinforcement learning,'' 1998.

\bibitem{6386109}
E.~{Todorov}, T.~{Erez}, and Y.~{Tassa}, ``Mujoco: A physics engine for
  model-based control,'' in \emph{2012 IEEE/RSJ International Conference on
  Intelligent Robots and Systems}, 2012, pp. 5026--5033.

\bibitem{Brockman2016OpenAIG}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba, ``Openai gym,'' \emph{ArXiv}, vol. abs/1606.01540, 2016.

\bibitem{tensorforce}
\BIBentryALTinterwordspacing
A.~Kuhnle, M.~Schaarschmidt, and K.~Fricke, ``Tensorforce: a tensorflow library
  for applied reinforcement learning,'' Web page, 2017. [Online]. Available:
  \url{https://github.com/tensorforce/tensorforce}
\BIBentrySTDinterwordspacing

\bibitem{SpinningUp2018}
J.~Achiam, ``{Spinning Up in Deep Reinforcement Learning},'' 2018.

\end{thebibliography}
